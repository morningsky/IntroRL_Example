{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动态规划 策略评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.1 with Exercises 4.1- 4.3 and Figure 4.1\n",
    "\n",
    "### 表格世界问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gridworld\n",
    "from gridworld import GridWorldEnv #基于gym的通用格子环境\n",
    "import numpy as np\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniGridWorld():\n",
    "    '''\n",
    "    4*4的一个格子世界环境 \n",
    "    T  o  o  o\n",
    "    o  x  o  o\n",
    "    o  o  o  o\n",
    "    o  o  o  T\n",
    "    1.终点reward为0，到达其他点reward都为-1\n",
    "    2.随机设置初始状态x\n",
    "    3.终点在左上角和右下角\n",
    "    4.0,1,2,3分布代表 ← → ↑ ↓\n",
    "    \n",
    "    坐标计算方法： 左下角为(0,0) 两个终点(0,3) (3,0)  跟书上的计数不大一样 书上是从左上到右下计数\n",
    "    '''\n",
    "    env = GridWorldEnv(n_width=4,\n",
    "                       n_height=4,\n",
    "                       u_size=40,\n",
    "                       default_reward=-1, #默认reward\n",
    "                       default_type=0,\n",
    "                       windy=False) # 类型为1的格子为障碍格子，不可进入 无风\n",
    "    env.start = (0,0) #注意是元组 不是列表！！！\n",
    "    env.ends = [(0, 3), (3, 0)]\n",
    "    #env.rewards = [(0, 3, 0), (3, 0, 0)] #设置起点终点rewad为0\n",
    "    env.refresh_setting()\n",
    "    return env\n",
    "\n",
    "##基于gym实现的环境没有状态转移矩阵P[s][a]  这里由于所有的问题基本上除了终止态转移概率为0，\n",
    "##其他任何的s-a-s' pair转移概率都为1  所以迭代过程中手动处理终止态的V值即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, nS, nA, env, discount_factor=1.0, theta=0.00001): #\n",
    "    \"\"\"\n",
    "        policy evaluation:  给定策略 使value function收敛，只迭代一次，获得一个不那么精确的V\n",
    "        policy improvement: 给定V，求解最优的action，优化策略\n",
    "    \"\"\"\n",
    "    V = np.zeros(nS)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(nS):\n",
    "            if s in [env._xy_to_state(i) for i in env.ends]: #如果s是终止态，则所有的v[s]=0\n",
    "                V[s] = 0\n",
    "                continue\n",
    "            v = 0\n",
    "            for a in range(nA):\n",
    "                env.state = s\n",
    "                next_state, reward, done, info = env.step(a)\n",
    "                v += policy[s][a]  * (reward + discount_factor * V[next_state])\n",
    "                                \n",
    "            delta = max(delta, np.abs(v - V[s])) \n",
    "            V[s] = v\n",
    " \n",
    "        if delta < theta: #在一次迭代中，v与v'的最大差值如果小于theta 则收敛\n",
    "            break\n",
    "    return np.around(V)\n",
    "\n",
    "\n",
    "def draw_greedy_policy(V, env, nS, nA, discount_factor=1): #给定V 取贪心策略的结果可视化\n",
    "    greedy_policy = []  \n",
    "    for s in range(nS):\n",
    "        if s in [env._xy_to_state(i) for i in env.ends]: #如果s是终止态，则所有的q[s,a]=0\n",
    "            greedy_policy.append(nA) #标记终止态\n",
    "            continue\n",
    "        values = np.zeros(nA)\n",
    "        for a in range(nA):\n",
    "            env.state = s\n",
    "            next_state, reward, done, info = env.step(a)\n",
    "            values[a] = reward + discount_factor * V[next_state]\n",
    "        greedy_policy.append(np.argmax(values)) #贪心策略选取最大值\n",
    "    \n",
    "    #可视化\n",
    "    pattern = {0:'←', 1:'→', 2:'↑', 3:'↓', 4:'O'}\n",
    "    policy_vis = [pattern[x] if x in pattern else x for x in greedy_policy]\n",
    "    print(np.array(policy_vis).reshape((4,4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MiniGridWorld()\n",
    "nS =  env.observation_space.n\n",
    "nA = env.action_space.n\n",
    "\n",
    "random_policy = np.ones([nS, nA]) / nA\n",
    "V = policy_eval(random_policy, nS, nA, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-22. -20. -14.   0.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-14. -18. -20. -20.]\n",
      " [  0. -14. -20. -22.]]\n"
     ]
    }
   ],
   "source": [
    "print(V.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['→' '→' '→' 'O']\n",
      " ['↑' '→' '→' '↓']\n",
      " ['↑' '←' '←' '↓']\n",
      " ['O' '←' '←' '←']]\n"
     ]
    }
   ],
   "source": [
    "draw_greedy_policy(V, env, nS, nA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
